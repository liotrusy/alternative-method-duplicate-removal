{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Experiment: Alternative approach to remove duplicate entries\n",
    "\n",
    "\n",
    "## 1. Goal: Reduct the number of loops used to remove duplicate entries\n",
    "\n",
    "The goal of the experiment is to attempt to **reduce the number of loops used to remove duplicate entries from a dataset**. The final output is a list of unique app entries. In other words, we are trying to identify unique apps and separating the duplicate entries\n",
    "\n",
    "This is [data set](https://www.kaggle.com/lava18/google-play-store-apps) used for the analys. I contains data about approx. 10,000 Android apps from Google Play. The data was collected in August 2018.\n",
    "\n",
    "First of all let's start by laying some assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1. Premises and assumptions\n",
    "\n",
    "Considering my current stage of programming/data science training, I'll have some constraints as to the kind of objects and operations I can employ to solve the problem. Maybe there are even better ways to solve the problem, so I'd be interested in hearing your opinion about them.\n",
    "\n",
    "Anyways, now we can start by exploring the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform a csv file into a list of list\n",
    "def csv_to_list(file_name):\n",
    "    # opening the file\n",
    "    opened_file = open(file_name,encoding='utf8')\n",
    "    \n",
    "    # transforming the file output in a lsit of lists\n",
    "    from csv import reader\n",
    "    read_file = reader(opened_file)\n",
    "    data_set = list(read_file)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the csv files (datasets) into list of lists\n",
    "apps_google = csv_to_list(\"googleplaystore.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have transformed the datasets into list of lists, we explore the headers of the datasets to ensure that we have all the relevant attributes (columns) needed to perform the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['App',\n",
       " 'Category',\n",
       " 'Rating',\n",
       " 'Reviews',\n",
       " 'Size',\n",
       " 'Installs',\n",
       " 'Type',\n",
       " 'Price',\n",
       " 'Content Rating',\n",
       " 'Genres',\n",
       " 'Last Updated',\n",
       " 'Current Ver',\n",
       " 'Android Ver']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apps_google[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows us to explore the number of columns, rows and datapoints\n",
    "def explore_dataset(dataset):\n",
    "    \n",
    "    dataset_rows = len(dataset[1:])\n",
    "    dataset_columns = len(dataset[0])\n",
    "    \n",
    "    number_of_datapoints = dataset_rows * dataset_columns\n",
    "    message = \"The dataset has {} columns, {:,} rows and {:,} datapoints\".format(dataset_columns, dataset_rows, number_of_datapoints)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 13 columns, 10,841 rows and 140,933 datapoints\n"
     ]
    }
   ],
   "source": [
    "explore_dataset(apps_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need to ensure that there are no wrong data and remove duplicates.\n",
    "\n",
    "----\n",
    "\n",
    "## 3. Remove inaccurate data\n",
    "\n",
    "The first step is to verify that each row has the required amount of colums (13) based on the exploration we have done above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10472,\n",
       "  ['Life Made WI-Fi Touchscreen Photo Frame',\n",
       "   '1.9',\n",
       "   '19',\n",
       "   '3.0M',\n",
       "   '1,000+',\n",
       "   'Free',\n",
       "   '0',\n",
       "   'Everyone',\n",
       "   '',\n",
       "   'February 11, 2018',\n",
       "   '1.0.19',\n",
       "   '4.0 and up'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isolating header and calculating its length, and separating the dataset\n",
    "header = apps_google[0]\n",
    "header_length = len(header)\n",
    "dataset = apps_google[1:]\n",
    "\n",
    "# isolating the apps (rows and index) that are different length compared to the header\n",
    "different_length_rows = []\n",
    "\n",
    "for row in enumerate(dataset):\n",
    "    # counting the number of columns of the app\n",
    "    app_details = row[1]\n",
    "    number_of_columns = len(app_details)\n",
    "    \n",
    "    # attach the different rows to the different_length_rows list\n",
    "    if number_of_columns != header_length:\n",
    "        different_length_rows.append(row)\n",
    "\n",
    "different_length_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row 10,472 is missing a column. This means that is has to be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the index we need to target to remove contents\n",
    "index_to_remove = different_length_rows[0][0]\n",
    "\n",
    "# deleting the faulty entry from the dataset\n",
    "del dataset[index_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 4. The Experiment: Removing duplicate entries\n",
    "\n",
    "### 4.1. The Original Solution\n",
    "\n",
    "The original solution can be found [here](https://github.com/dataquestio/solutions/blob/master/Mission350Solutions.ipynb).\n",
    "\n",
    "The authors used the following process to arrive at the list of unique app entries.\n",
    "\n",
    "**STEP 1: Separating unique and duplicate names**\n",
    "\n",
    "With the original solution we create two separate lists containing the app names that are either unique (unique_apps) or duplicate (duplicate_apps).\n",
    "\n",
    "![image.png](https://data-portfolio-images.s3.eu-central-1.amazonaws.com/Screenshot_2020-06-11+dataquestio+solutions.png)\n",
    "\n",
    "And, we also have 9,659 unique names.\n",
    "\n",
    "Loops over a list with 10,840 items.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2: Create a dictionary with unique keys and, as value, the highest number of reviews**\n",
    "\n",
    "![image.png](https://data-portfolio-images.s3.eu-central-1.amazonaws.com/Screenshot_2020-06-11+dataquestio+solutions(1).png)\n",
    "\n",
    "Loops over a list with 10,840 items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3: Creating a list of unique entries**\n",
    "\n",
    "![image.png](https://data-portfolio-images.s3.eu-central-1.amazonaws.com/Screenshot_2020-06-11+dataquestio+solutions(2).png)\n",
    "\n",
    "\n",
    "Loops over a list with 10,840 items.\n",
    "\n",
    "FINAL RESULT: To create the list of unique entries the authors have over 35,520 items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 4.2. My Experiment\n",
    "\n",
    "My goal was to attempt to find a way to reduce the number of loops involved in the creation of the list of unique app entries with the highest number of reviews. The reason why we focus on the highest number of reviews is that, in oder to proced with the analysis, we must select a criteria to select one option for each set of duplicates - in order to avoid random selection of the entries.\n",
    "\n",
    "\n",
    "#### Step 1 â€“ Separate unique and duplicate entries\n",
    "\n",
    "The thought process I had was to elaborate the data to create reusable parts (better inputs for the following process). So, the first thing I did was to the focus in my approach on both entries and names, and not only on the names. With this approach, I create two sets of input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1,181 duplicates\n"
     ]
    }
   ],
   "source": [
    "# creating a list for duplicates\n",
    "duplicate_apps = []\n",
    "\n",
    "# creating a list for unique app names, and a clean list of lists with unique names\n",
    "unique_apps = {}\n",
    "\n",
    "# sorting the dataset\n",
    "for app in dataset:\n",
    "    app_name = app[0]\n",
    "    if app_name in unique_apps:\n",
    "        duplicate_apps.append(app)\n",
    "    else:\n",
    "        unique_apps[app_name] = app\n",
    "\n",
    "# printing the number of duplicate entries\n",
    "number_of_duplicate_apps = len(duplicate_apps)\n",
    "print(\"There are {:,} duplicates\".format(number_of_duplicate_apps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A list of all the duplicate entries\n",
    "* A dictionary of unique names (the keys), and the first entry for the unique name (as the value). This will still allow me to use the in operator to search the keys but has the advantage of having the entry attached.\n",
    "\n",
    "\n",
    "To better examine the situation, here is an example of a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quick PDF Scanner + OCR FREE', 'BUSINESS', '4.2', '80805', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'February 26, 2018', 'Varies with device', '4.0.3 and up']\n",
      "['Quick PDF Scanner + OCR FREE', 'BUSINESS', '4.2', '80805', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'February 26, 2018', 'Varies with device', '4.0.3 and up']\n",
      "['Quick PDF Scanner + OCR FREE', 'BUSINESS', '4.2', '80804', 'Varies with device', '5,000,000+', 'Free', '0', 'Everyone', 'Business', 'February 26, 2018', 'Varies with device', '4.0.3 and up']\n"
     ]
    }
   ],
   "source": [
    "duplicate_app_name = duplicate_apps[0][0]\n",
    "\n",
    "for app in dataset:\n",
    "    app_name = app[0]\n",
    "    if app_name == duplicate_app_name:\n",
    "        print(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9,659 unique names\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {:,} unique names\".format(len(unique_apps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "At this point we have\n",
    "\n",
    "* 9,659 unique names\n",
    "* 1,181 duplicate entries\n",
    "\n",
    "\n",
    "#### Step 2 â€“ updating the dictionary with duplicate entries that have a higher number of reviews\n",
    "\n",
    "Because I have already isolated the data I need for this step in the previous one, I can focus the work only on the duplicate entries instead of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loops update unique_apps entries if the duplicate has more reviews\n",
    "for duplicate in duplicate_apps:\n",
    "    \n",
    "    # extracting the relevant information for the duplicate\n",
    "    duplicate_name = duplicate[0]\n",
    "    duplicate_reviews = int(duplicate[3])\n",
    "    \n",
    "    # extracting the reviews info about existing entry in unique_app dictionary\n",
    "    unique_app_reviews = unique_apps[duplicate_name][3]\n",
    "    unique_app_reviews = int(unique_app_reviews)\n",
    "    \n",
    "    # updating the entry if the duplicate has higher reviews\n",
    "    if duplicate_reviews > unique_app_reviews:\n",
    "        unique_apps[duplicate_name] = duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've looped over a list with 1,181 items.\n",
    "\n",
    "At the end of this process we have an updated dictionary with unique keys and, as values, entries that have the highest number of ratings in the dataset.\n",
    "\n",
    "\n",
    "#### Step 3 â€“ creating a list of unique entries\n",
    "\n",
    "To create the final list of unique values, we can loop over the dictionary and extract the entry for each unique key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crating the cleaned list of unique app entries\n",
    "apps_google_duplicateless = []\n",
    "\n",
    "for app_name in unique_apps:\n",
    "        entry = unique_apps[app_name]\n",
    "        apps_google_duplicateless.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've looped over a dictionary of 9,659 items and we've created the desired output.\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "By comparing the two approaches, we can see that the:\n",
    "\n",
    "| Approach | Step 1 |  Step 2 | Step 3 | Step 4 |\n",
    "|------|-----|-----|----|----|\n",
    "| Original approach | 10,840 |  10,840 | 10,840 | 32,520 |\n",
    "| Original approach | 10,840 |  1,191 | 9,659 | 21,680 |\n",
    " \n",
    "SUMMARY:\n",
    "\n",
    "* Loops saved: 10,840\n",
    "* Improvement: 1,5x\n",
    "\n",
    "This approach has saved an entire iteration over the whole dataset - it seems that I may have found a solution that is more efficient. I can't say at 100% because I haven't looked at speed or memory performance.\n",
    "\n",
    "Loved running this experiemnt!\n",
    "\n",
    "Thanks for reading. And, I you have some opinion, comments or feedback on this approach don't hesitate to reach out! Cheers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
